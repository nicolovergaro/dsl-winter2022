{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSL winter assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import of all necessary libraries"
      ],
      "metadata": {
        "id": "2fi5GP96DLit"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSN-lTq-WvKS"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import seaborn as sns\n",
        "sns.set(rc={'figure.figsize':(11.7,8.27)})\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Function for preprocessing\n",
        "def clean_dataframe(df, ohe_date=None):\n",
        "  # FLAG è uguale per tutti\n",
        "    df_new = df.drop(columns=['flag'])\n",
        "\n",
        "    substitutions = {\n",
        "        '&quot;': '\"',\n",
        "        '&amp;': '&',\n",
        "        '&gt;': '>',\n",
        "        '&lt;': '<',\n",
        "        '<3': 'love'\n",
        "        }\n",
        "    \n",
        "    for abb, description in substitutions.items():\n",
        "        df_new['text'] = df_new['text'].str.replace(abb, description, regex=False)\n",
        "        \n",
        "    \n",
        "    tweet_length = df['text'].str.len()\n",
        "    \n",
        "    # Removing links\n",
        "    regex = '(www\\.)|(https?:\\/\\/)[-a-zA-Z0-9@:%._\\+~#=\\/]*'\n",
        "    contains_links = df_new['text'].str.contains(regex)\n",
        "    df_new['text'] = df_new['text'].str.replace(regex,'',regex=True)\n",
        "\n",
        "    # removing hashtags\n",
        "    regex='#\\w+'\n",
        "    df_new['text'] = df_new['text'].str.replace(regex,'',regex=True)\n",
        "    \n",
        "    # count number of mentions\n",
        "    regex = '@\\w+' \n",
        "    contains_mentions = df_new['text'].str.contains(regex)\n",
        "    \n",
        "  \n",
        "    EMOTICONS_EMO = {\n",
        "        u\":‑)\":\"Happy face or smiley\",\n",
        "        u\":-)))\":\"Very very Happy face or smiley\",\n",
        "        u\":)\":\"Happy face or smiley\",\n",
        "        u\":))\":\"Very Happy face or smiley\",\n",
        "        u\":)))\":\"Very very Happy face or smiley\",\n",
        "        u\":-]\":\"Happy face or smiley\",\n",
        "        u\":]\":\"Happy face or smiley\",\n",
        "        u\":-3\":\"Happy face smiley\",\n",
        "        u\":3\":\"Happy face smiley\",\n",
        "        u\":->\":\"Happy face smiley\",\n",
        "        u\":>\":\"Happy face smiley\",\n",
        "        u\"8-)\":\"Happy face smiley\",\n",
        "        u\":o)\":\"Happy face smiley\",\n",
        "        u\":-}\":\"Happy face smiley\",\n",
        "        u\":}\":\"Happy face smiley\",\n",
        "        u\":-)\":\"Happy face smiley\",\n",
        "        u\":c)\":\"Happy face smiley\",\n",
        "        u\":^)\":\"Happy face smiley\",\n",
        "        u\"=]\":\"Happy face smiley\",\n",
        "        u\"=)\":\"Happy face smiley\",\n",
        "        u\":‑D\":\"Laughing, big grin or laugh with glasses\",\n",
        "        u\":D\":\"Laughing, big grin or laugh with glasses\",\n",
        "        u\"8‑D\":\"Laughing, big grin or laugh with glasses\",\n",
        "        u\"8D\":\"Laughing, big grin or laugh with glasses\",\n",
        "        u\"X‑D\":\"Laughing, big grin or laugh with glasses\",\n",
        "        u\"XD\":\"Laughing, big grin or laugh with glasses\",\n",
        "        u\"=D\":\"Laughing, big grin or laugh with glasses\",\n",
        "        u\"=3\":\"Laughing, big grin or laugh with glasses\",\n",
        "        u\"B^D\":\"Laughing, big grin or laugh with glasses\",\n",
        "        u\":-))\":\"Very happy\",\n",
        "        u\":-(\":\"Frown, sad, andry or pouting\",\n",
        "        u\":‑(\":\"Frown, sad, andry or pouting\",\n",
        "        u\":(\":\"Frown, sad, andry or pouting\",\n",
        "        u\":‑c\":\"Frown, sad, andry or pouting\",\n",
        "        u\":c\":\"Frown, sad, andry or pouting\",\n",
        "        u\":‑<\":\"Frown, sad, andry or pouting\",\n",
        "        u\":<\":\"Frown, sad, andry or pouting\",\n",
        "        u\":‑[\":\"Frown, sad, andry or pouting\",\n",
        "        u\":[\":\"Frown, sad, andry or pouting\",\n",
        "        u\":-||\":\"Frown, sad, andry or pouting\",\n",
        "        u\">:[\":\"Frown, sad, andry or pouting\",\n",
        "        u\":{\":\"Frown, sad, andry or pouting\",\n",
        "        u\":@\":\"Frown, sad, andry or pouting\",\n",
        "        u\">:(\":\"Frown, sad, andry or pouting\",\n",
        "        u\":'‑(\":\"Crying\",\n",
        "        u\":'(\":\"Crying\",\n",
        "        u\":'‑)\":\"Tears of happiness\",\n",
        "        u\":')\":\"Tears of happiness\",\n",
        "        u\"D‑':\":\"Horror\",\n",
        "        u\"D:<\":\"Disgust\",\n",
        "        u\"D:\":\"Sadness\",\n",
        "        u\"D8\":\"Great dismay\",\n",
        "        u\"D;\":\"Great dismay\",\n",
        "        u\"D=\":\"Great dismay\",\n",
        "        u\"DX\":\"Great dismay\",\n",
        "        u\":‑O\":\"Surprise\",\n",
        "        u\":O\":\"Surprise\",\n",
        "        u\":‑o\":\"Surprise\",\n",
        "        u\":o\":\"Surprise\",\n",
        "        u\":-0\":\"Shock\",\n",
        "        u\"8‑0\":\"Yawn\",\n",
        "        u\">:O\":\"Yawn\",\n",
        "        u\":-*\":\"Kiss\",\n",
        "        u\":*\":\"Kiss\",\n",
        "        u\":X\":\"Kiss\",\n",
        "        u\";‑)\":\"Wink or smirk\",\n",
        "        u\";)\":\"Wink or smirk\",\n",
        "        u\"*-)\":\"Wink or smirk\",\n",
        "        u\"*)\":\"Wink or smirk\",\n",
        "        u\";‑]\":\"Wink or smirk\",\n",
        "        u\";]\":\"Wink or smirk\",\n",
        "        u\";^)\":\"Wink or smirk\",\n",
        "        u\":‑,\":\"Wink or smirk\",\n",
        "        u\";D\":\"Wink or smirk\",\n",
        "        u\":‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "        u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "        u\"X‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "        u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "        u\":‑Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "        u\":Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "        u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "        u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "        u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "        u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "        u\":‑/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "        u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "        u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "        u\">:[(\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "        u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "        u\":[(\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "        u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "        u\"=[(\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "        u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "        u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "        u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "        u\":‑|\":\"Straight face\",\n",
        "        u\":|\":\"Straight face\",\n",
        "        u\":$\":\"Embarrassed or blushing\",\n",
        "        u\":‑x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "        u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "        u\":‑#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "        u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "        u\":‑&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "        u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "        u\"O:‑)\":\"Angel, saint or innocent\",\n",
        "        u\"O:)\":\"Angel, saint or innocent\",\n",
        "        u\"0:‑3\":\"Angel, saint or innocent\",\n",
        "        u\"0:3\":\"Angel, saint or innocent\",\n",
        "        u\"0:‑)\":\"Angel, saint or innocent\",\n",
        "        u\"0:)\":\"Angel, saint or innocent\",\n",
        "        u\":‑b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "        u\"0;^)\":\"Angel, saint or innocent\",\n",
        "        u\">:‑)\":\"Evil or devilish\",\n",
        "        u\">:)\":\"Evil or devilish\",\n",
        "        u\"}:‑)\":\"Evil or devilish\",\n",
        "        u\"}:)\":\"Evil or devilish\",\n",
        "        u\"3:‑)\":\"Evil or devilish\",\n",
        "        u\"3:)\":\"Evil or devilish\",\n",
        "        u\">;)\":\"Evil or devilish\",\n",
        "        u\"|;‑)\":\"Cool\",\n",
        "        u\"|‑O\":\"Bored\",\n",
        "        u\":‑J\":\"Tongue-in-cheek\",\n",
        "        u\"#‑)\":\"Party all night\",\n",
        "        u\"%‑)\":\"Drunk or confused\",\n",
        "        u\"%)\":\"Drunk or confused\",\n",
        "        u\":-###..\":\"Being sick\",\n",
        "        u\":###..\":\"Being sick\",\n",
        "        u\"<:‑|\":\"Dump\",\n",
        "        u\"(>_<)\":\"Troubled\",\n",
        "        u\"(>_<)>\":\"Troubled\",\n",
        "        u\"(';')\":\"Baby\",\n",
        "        u\"(^^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "        u\"(^_^;)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "        u\"(-_-;)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "        u\"(~_~;) (・.・;)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "        u\"(-_-)zzz\":\"Sleeping\",\n",
        "        u\"(^_-)\":\"Wink\",\n",
        "        u\"((+_+))\":\"Confused\",\n",
        "        u\"(+o+)\":\"Confused\",\n",
        "        u\"(o|o)\":\"Ultraman\",\n",
        "        u\"^_^\":\"Joyful\",\n",
        "        u\"(^_^)/\":\"Joyful\",\n",
        "        u\"(^O^)／\":\"Joyful\",\n",
        "        u\"(^o^)／\":\"Joyful\",\n",
        "        u\"(__)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "        u\"_(._.)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "        u\"<(_ _)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "        u\"<m(__)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "        u\"m(__)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "        u\"m(_ _)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "        u\"('_')\":\"Sad or Crying\",\n",
        "        u\"(/_;)\":\"Sad or Crying\",\n",
        "        u\"(T_T) (;_;)\":\"Sad or Crying\",\n",
        "        u\"(;_;\":\"Sad of Crying\",\n",
        "        u\"(;_:)\":\"Sad or Crying\",\n",
        "        u\"(;O;)\":\"Sad or Crying\",\n",
        "        u\"(:_;)\":\"Sad or Crying\",\n",
        "        u\"(ToT)\":\"Sad or Crying\",\n",
        "        u\";_;\":\"Sad or Crying\",\n",
        "        u\";-;\":\"Sad or Crying\",\n",
        "        u\";n;\":\"Sad or Crying\",\n",
        "        u\";;\":\"Sad or Crying\",\n",
        "        u\"Q.Q\":\"Sad or Crying\",\n",
        "        u\"T.T\":\"Sad or Crying\",\n",
        "        u\"QQ\":\"Sad or Crying\",\n",
        "        u\"Q_Q\":\"Sad or Crying\",\n",
        "        u\"(-.-)\":\"Shame\",\n",
        "        u\"(-_-)\":\"Shame\",\n",
        "        u\"(一一)\":\"Shame\",\n",
        "        u\"(；一_一)\":\"Shame\",\n",
        "        u\"(=_=)\":\"Tired\",\n",
        "        u\"(=^·^=)\":\"cat\",\n",
        "        u\"(=^··^=)\":\"cat\",\n",
        "        u\"=_^= \":\"cat\",\n",
        "        u\"(..)\":\"Looking down\",\n",
        "        u\"(._.)\":\"Looking down\",\n",
        "        u\"^m^\":\"Giggling with hand covering mouth\",\n",
        "        u\"(・・?\":\"Confusion\",\n",
        "        u\"(?_?)\":\"Confusion\",\n",
        "        u\">^_^<\":\"Normal Laugh\",\n",
        "        u\"<^!^>\":\"Normal Laugh\",\n",
        "        u\"^/^\":\"Normal Laugh\",\n",
        "        u\"（*^_^*）\" :\"Normal Laugh\",\n",
        "        u\"(^<^) (^.^)\":\"Normal Laugh\",\n",
        "        u\"(^^)\":\"Normal Laugh\",\n",
        "        u\"(^.^)\":\"Normal Laugh\",\n",
        "        u\"(^_^.)\":\"Normal Laugh\",\n",
        "        u\"(^_^)\":\"Normal Laugh\",\n",
        "        u\"(^^)\":\"Normal Laugh\",\n",
        "        u\"(^J^)\":\"Normal Laugh\",\n",
        "        u\"(*^.^*)\":\"Normal Laugh\",\n",
        "        u\"(^—^）\":\"Normal Laugh\",\n",
        "        u\"(#^.^#)\":\"Normal Laugh\",\n",
        "        u\"（^—^）\":\"Waving\",\n",
        "        u\"(;_;)/~~~\":\"Waving\",\n",
        "        u\"(^.^)/~~~\":\"Waving\",\n",
        "        u\"(-_-)/~~~ ($··)/~~~\":\"Waving\",\n",
        "        u\"(T_T)/~~~\":\"Waving\",\n",
        "        u\"(ToT)/~~~\":\"Waving\",\n",
        "        u\"(*^0^*)\":\"Excited\",\n",
        "        u\"(*_*)\":\"Amazed\",\n",
        "        u\"(*_*;\":\"Amazed\",\n",
        "        u\"(+_+) (@_@)\":\"Amazed\",\n",
        "        u\"(*^^)v\":\"Laughing,Cheerful\",\n",
        "        u\"(^_^)v\":\"Laughing,Cheerful\",\n",
        "        u\"((d[-_-]b))\":\"Headphones,Listening to music\",\n",
        "        u'(-\"-)':\"Worried\",\n",
        "        u\"(ーー;)\":\"Worried\",\n",
        "        u\"(^0_0^)\":\"Eyeglasses\",\n",
        "        u\"(＾ｖ＾)\":\"Happy\",\n",
        "        u\"(＾ｕ＾)\":\"Happy\",\n",
        "        u\"(^)o(^)\":\"Happy\",\n",
        "        u\"(^O^)\":\"Happy\",\n",
        "        u\"(^o^)\":\"Happy\",\n",
        "        u\")^o^(\":\"Happy\",\n",
        "        u\":O o_O\":\"Surprised\",\n",
        "        u\"o_0\":\"Surprised\",\n",
        "        u\"o.O\":\"Surpised\",\n",
        "        u\"(o.o)\":\"Surprised\",\n",
        "        u\"oO\":\"Surprised\",\n",
        "        u\"(*￣m￣)\":\"Dissatisfied\",\n",
        "        u\"(‘A`)\":\"Snubbed or Deflated\"\n",
        "    }\n",
        "    \n",
        "    print(\"Traduco emoticons\")\n",
        "    for abb, description in EMOTICONS_EMO.items():\n",
        "        df_new['text'] = df_new['text'].str.replace(abb, description, regex=False)\n",
        "        \n",
        "    \n",
        "     \n",
        "    print(\"Tokenizzo\")\n",
        "    tokenized_tweets = []\n",
        "    for tweet in df_new['text']:\n",
        "        tknzr = TweetTokenizer()\n",
        "        tokens = tknzr.tokenize(tweet)\n",
        "          \n",
        "        filtered_tweet = []\n",
        "        \n",
        "        for word in tokens:\n",
        "          word = re.sub(r'((.)\\2)\\2+', r'\\1', word.strip().lower())\n",
        "          if (word.isalnum() or \"'\" in word):\n",
        "            if word==\"don't\":\n",
        "              word='dont'\n",
        "            filtered_tweet.append(word)\n",
        "                \n",
        "        tokenized_tweets.append(filtered_tweet)\n",
        "\n",
        "    df_new['filtered'] = tokenized_tweets\n",
        "\n",
        "    df_new['filtered_text'] = [\" \".join(map(str,l)) for l in df_new['filtered']]\n",
        "    df_new = df_new.drop(columns=['filtered'])\n",
        "    \n",
        "    \n",
        "    #Parsing dates\n",
        "    df_new['day'] = [date[0] for date in df_new['date'].str.split()]\n",
        "    df_new['date-month'] = [date[1]+\" \"+date[2] for date in df_new['date'].str.split()]\n",
        "    df_new['hour'] = [int(date[3].split(':')[0]) for date in df_new['date'].str.split()]\n",
        "    \n",
        "    \n",
        "    \n",
        "    if ohe_date is None:\n",
        "        print(\"Inizializzo ohe_date\")\n",
        "        ohe_date = OneHotEncoder(sparse=False, dtype=int)\n",
        "        day_month = ohe_date.fit_transform(df_new[['day','date-month']])\n",
        "    else:\n",
        "        day_month = ohe_date.transform(df_new[['day','date-month']])\n",
        "        \n",
        "    df_new = df_new.drop(columns=['day','date-month', 'date'])\n",
        "    df_new = pd.concat([df_new, pd.DataFrame(day_month)], axis=1)\n",
        "    \n",
        "    #Add user information\n",
        "    df_new['filtered_text'] = df_new['filtered_text'] + \" \" + df['user']    \n",
        "    \n",
        "    df_new['length'] = tweet_length\n",
        "    df_new['contains_links'] = list(map(int, contains_links))\n",
        "    df_new['contains_mentions'] = list(map(int, contains_mentions))\n",
        "\n",
        "    \n",
        "    return df_new, ohe_date"
      ],
      "metadata": {
        "id": "tUXHGpEAEbvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('development.csv')\n",
        "cleaned, ohe_date = clean_dataframe(df)\n",
        "print(\"duplicates\")\n",
        "ids_to_remove = cleaned[cleaned['ids'].duplicated()]['ids']\n",
        "cleaned = cleaned[~cleaned['ids'].isin(ids_to_remove)]\n",
        "final = cleaned.drop(columns=['ids', 'user', 'length'])\n",
        "final = final.reset_index(drop=True)\n",
        "y = final['sentiment']\n",
        "final = final.drop(columns=['sentiment'])\n",
        "\n",
        "df_eval = pd.read_csv('evaluation.csv')\n",
        "cleaned_eval, ohe_date = clean_dataframe(df_eval, ohe_date)\n",
        "cleaned_eval = cleaned_eval.drop(columns=['ids', 'user', 'length'])"
      ],
      "metadata": {
        "id": "KItkF1z9Xu8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05aa02e6-8390-4478-f12f-9cb882f4ec0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/strings.py:2001: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
            "  return func(self, *args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traduco emoticons\n",
            "Tokenizzo\n",
            "Inizializzo ohe_date\n",
            "duplicates\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params= {\n",
        "    'ngram_range': [(1,2),(1,3)],\n",
        "    'max_df': [0.1, 0.2, 0.4, 0.8],\n",
        "    'min_df': [2, 5, 30]\n",
        "    }\n",
        "  \n",
        "best_config={}\n",
        "best_f1 = 0.0\n",
        "for config in ParameterGrid(params):\n",
        "  print(f\"Current configuration: {config}\")\n",
        "  vectorizer = TfidfVectorizer(**config)\n",
        "  X = vectorizer.fit_transform(cleaned['filtered_text'])\n",
        "  X = hstack([X, cleaned.iloc[:, 2:]])\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  model = LinearSVC()\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  f1 = f1_score(y_test, y_pred, average='macro')\n",
        "  print(f\"F1: {f1}\")\n",
        "  if f1 > best_f1:\n",
        "    best_f1 = f1\n",
        "    best_config = config   \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raqufZ_XXBNT",
        "outputId": "11dffb95-0883-47c4-fab4-108810539d1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current configuration: {'max_df': 0.4, 'min_df': 2, 'ngram_range': (1, 3)}\n",
            "F1: 0.8474193509953176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_config)\n",
        "print(best_f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx4vnFBLXKsk",
        "outputId": "9d6504cd-ab1a-4c01-f881-1ea6349a59aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'max_df': 0.1, 'min_df': 5, 'ngram_range': (1, 2)}\n",
            "0.8303882160234863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_valid, X_test,y_train_valid, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
        "# interval = np.arange(0.45, 0.65, 0.01)\n",
        "interval = np.arange(0.1, 0.1, 0.1)\n",
        "params_svc = {'C': interval,\n",
        "              'random_state': [42],\n",
        "              'max_iter': [10000],\n",
        "              'penalty': ['l1', 'l2']}\n",
        "f1_scores=[]\n",
        "kf = KFold(5)\n",
        "for config in ParameterGrid(params_svc):\n",
        "  model_f1 = []\n",
        "  counts = []\n",
        "  for train_indices, valid_indices in kf.split(X_train_valid):\n",
        "    X_train = X_train_valid[train_indices]\n",
        "    y_train = y_train_valid.iloc[train_indices]\n",
        "    X_valid = X_train_valid[valid_indices]\n",
        "    y_valid = y_train_valid.iloc[valid_indices]\n",
        "        \n",
        "    # keep track of the number of elements in each split\n",
        "    counts.append(len(train_indices)) \n",
        "        \n",
        "    model = LinearSVC(**config)\n",
        "    model.fit(X_train, y_train)\n",
        "    f1 = f1_score(y_valid, model.predict(X_valid), average='macro')\n",
        "    model_f1.append(f1)\n",
        "  f1_scores.append(np.average(model_f1, weights=counts))\n"
      ],
      "metadata": {
        "id": "hg7iuNPoCkEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_scores\n",
        "list(ParameterGrid(params_svc))[np.argmax(f1_scores)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXJj3eSOjyrx",
        "outputId": "fe7b1c35-a49f-4c21-8f68-2635982ff501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 0.5, 'max_iter': 10000, 'random_state': 42}"
            ]
          },
          "metadata": {},
          "execution_count": 275
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_config = list(ParameterGrid(params_svc))[np.argmax(f1_scores)]\n",
        "best_model = LinearSVC(**best_config)\n",
        "best_model.fit(X_train_valid, y_train_valid)\n",
        "f1_score(y_test, best_model.predict(X_test), average='macro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao6UHw7tHru3",
        "outputId": "2b93b322-f07a-4b1e-ab3a-481fa8a5fec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.844675485197101"
            ]
          },
          "metadata": {},
          "execution_count": 276
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_scores_plot = f1_scores\n",
        "interval_plot = interval\n",
        "list(ParameterGrid(params_svc))[np.argmax(f1_scores)]\n",
        "plt.plot(interval[:-1], f1_scores)"
      ],
      "metadata": {
        "id": "6LVLmwY9IfqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LINEAR SVC\n",
        "optimal_config_vect={'max_df': 0.4, 'min_df': 2, 'ngram_range': (1, 3)}\n",
        "vectorizer = TfidfVectorizer(**optimal_config_vect)\n",
        "X = vectorizer.fit_transform(final['filtered_text'])\n",
        "X = hstack([X, final.iloc[:, 2:]])\n",
        "X_test_eval = vectorizer.transform(cleaned_eval['filtered_text'])\n",
        "X_test_eval = hstack([X_test_eval, cleaned_eval.iloc[:, 2:]])\n",
        "\n",
        "optimal_config_model = {'C': 0.5,'max_iter': 10000}\n",
        "model = LinearSVC(**optimal_config_model)\n",
        "print(\"Prediction - evaluation\")\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X_test_eval)\n",
        "ids = range(0,74999)\n",
        "subm = pd.DataFrame(y_pred, columns=['Predicted'])\n",
        "subm['Id'] = ids\n",
        "subm.to_csv('submission16.csv', index=None)\n",
        "\n",
        "# COMPLEMENT NAIVE BAYES\n",
        "optimal_config_vect = {'max_df': 0.1, 'min_df': 5, 'ngram_range': (1, 2)}\n",
        "vectorizer = TfidfVectorizer(**optimal_config_vect)\n",
        "X = vectorizer.fit_transform(final['filtered_text'])\n",
        "X = hstack([X, final.iloc[:, 2:]])\n",
        "X_test_eval = vectorizer.transform(cleaned_eval['filtered_text'])\n",
        "X_test_eval = hstack([X_test_eval, cleaned_eval.iloc[:, 2:]])\n",
        "\n",
        "optimal_config_model = {'alpha': 0.3,'norm': False}\n",
        "model = ComplementNB(**optimal_config_model)\n",
        "print(\"Prediction - evaluation\")\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X_test_eval)\n",
        "ids = range(0,74999)\n",
        "subm = pd.DataFrame(y_pred, columns=['Predicted'])\n",
        "subm['Id'] = ids\n",
        "subm.to_csv('submission12_new.csv', index=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AzXJ1jXKAnI",
        "outputId": "323c1676-6716-4e88-c4f1-e02d474e2c77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction - evaluation\n",
            "Prediction - evaluation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wordcloud\n",
        "index_to_remove = final[final['filtered_text'].str.contains('add everyone')].index\n",
        "final = final.drop(index_to_remove)\n",
        "text = \" \".join(corpus for corpus in final['filtered_text'])\n",
        "stopwords = set(STOPWORDS)\n",
        "# wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", color_func=color_word).generate(text)\n",
        "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", max_words=180, width=1500, height=1000).generate(text)\n",
        "\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X9GUhQzBdn8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys = wordcloud.words_.keys()\n",
        "substitutions = {'uneasy hesitant': 'uneasy or hesitant',\n",
        "                 'wink smirk':'wink or smirk'}\n",
        "positivity = {}\n",
        "for key in keys:\n",
        "  #calculate positivity ratio\n",
        "  if key in substitutions: \n",
        "    counts = final[final['filtered_text'].str.contains(substitutions[key])]['sentiment'].value_counts()\n",
        "  else:\n",
        "    counts = final[final['filtered_text'].str.contains(key)]['sentiment'].value_counts()\n",
        "\n",
        "  ratio = counts[1] / counts[0]\n",
        "  positivity[key] = ratio\n",
        "\n",
        "#min-max normalization\n",
        "min_val = min(positivity.values())\n",
        "max_val = max(positivity.values())\n",
        "\n",
        "scaled_pos = {k: (v-min_val)/(max_val-min_val) for k,v in positivity.items()}\n",
        "sorted_words = [k for k, v in sorted(scaled_pos.items(), key=lambda item: item[1])]\n",
        "sorted_values = sorted(list(scaled_pos.values()))\n",
        "cmap=plt.get_cmap('seismic')\n",
        "colors=cmap(sorted_values)\n",
        "\n",
        "def color_word(word, font_size, position, orientation, random_state=None,**kwargs):\n",
        "  return matplotlib.colors.rgb2hex(colors[sorted_words.index(word)])\n",
        "\n",
        "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", max_words=180, color_func=color_word, width=1500, height=1000).generate(text)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.savefig('wordcloud.png',bbox_inches='tight')\n",
        "\n"
      ],
      "metadata": {
        "id": "LE63io5mvGye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize dataset\n",
        "optimal_config_vect={'max_df': 0.4, 'min_df': 2, 'ngram_range': (1, 3)}\n",
        "vectorizer = TfidfVectorizer(**optimal_config_vect)\n",
        "X = vectorizer.fit_transform(final['filtered_text'])\n",
        "X = hstack([X, final.iloc[:, 2:]])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "68BQJRSQbhaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear svc, without tuning"
      ],
      "metadata": {
        "id": "u26KAiacIZYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LinearSVC()\n",
        "clf.fit(X_train, y_train)\n",
        "print(f\"F1:  {f1_score(y_test, clf.predict(X_test), average='macro')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpqkGGauIncu",
        "outputId": "10a2af39-5ba4-4802-c6e6-6bc9f5dcfc3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1:  0.8492588607055161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other classifier, random forest, without tuning"
      ],
      "metadata": {
        "id": "vwp3mNN3hWiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Without tuning 0.8437427856013923 in 1.5h\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "print(f\"F1:  {f1_score(y_test, clf.predict(X_test), average='macro')}\")"
      ],
      "metadata": {
        "id": "SvzTfcHqhY_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4nO6g_FVIfVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Without tuning, 0.7892598078310736\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "print(f\"F1:  {f1_score(y_test, clf.predict(X_test), average='macro')}\")"
      ],
      "metadata": {
        "id": "NFiwtYdkFjVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other classifier, Complement Naive Bayes."
      ],
      "metadata": {
        "id": "GGOJ2_AnX23_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Without tuning\n",
        "clf = ComplementNB()\n",
        "clf.fit(X_train, y_train)\n",
        "print(f\"F1:  {f1_score(y_test, clf.predict(X_test), average='macro')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MZwn7mCX5HR",
        "outputId": "527ff1f2-e651-47b1-8265-c7cecabdb121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1:  0.8332333210431179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dataset for tuning\n",
        "X_train_valid, X_test,y_train_valid, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "interval = np.arange(0.0, 2, 0.1)\n",
        "params_CNB = {'alpha': interval,\n",
        "              'norm': [True, False]}\n",
        "nb = ComplementNB()\n",
        "clf = GridSearchCV(nb, params_CNB, scoring='f1_macro')\n",
        "clf.fit(X_train_valid, y_train_valid)\n",
        "print(f\"F1 on test: {f1_score(y_test, clf.predict(X_test), average='macro')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVb6GcXebSSw",
        "outputId": "b05b87af-245c-4978-b58d-6d39a3fccd90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 on test: 0.83075618194781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTCsC4QcUX2l",
        "outputId": "c26e6504-41a6-4d50-e74b-b950a493985f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alpha': 0.30000000000000004, 'norm': False}"
            ]
          },
          "metadata": {},
          "execution_count": 255
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['sentiment']\n",
        "df = df.drop(columns=['sentiment'])\n",
        "\n",
        "all_eval = pd.read_csv('evaluation.csv')"
      ],
      "metadata": {
        "id": "YKlKQPw-xRuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vect_naive = TfidfVectorizer()\n",
        "X = vect_naive.fit_transform(df['text'])\n",
        "X_eval = vect_naive.transform(all_eval['text'])\n",
        "model = LinearSVC()\n",
        "model.fit(X,y)\n",
        "y_pred = model.predict(X_eval)\n",
        "ids = range(0,74999)\n",
        "subm = pd.DataFrame(y_pred, columns=['Predicted'])\n",
        "subm['Id'] = ids\n",
        "subm.to_csv('submission13_svc_naive.csv', index=None)"
      ],
      "metadata": {
        "id": "CqI6N1QvWA_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import bernoulli\n",
        "prob_pos = y.value_counts()[1] / all_develop.shape[0]\n",
        "y_pred = bernoulli.rvs(prob_pos, size=all_eval.shape[0])\n",
        "ids = range(0,74999)\n",
        "subm = pd.DataFrame(y_pred, columns=['Predicted'])\n",
        "subm['Id'] = ids\n",
        "subm.to_csv('submission14_random_naive.csv', index=None)"
      ],
      "metadata": {
        "id": "DxAl1229WQrP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}